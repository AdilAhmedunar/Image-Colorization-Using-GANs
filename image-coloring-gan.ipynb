{"cells":[{"cell_type":"markdown","metadata":{"id":"famx1NQr192f"},"source":["## **Dependencies**"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":7456,"status":"ok","timestamp":1704706045347,"user":{"displayName":"Adil Ahmed","userId":"05674163524922588142"},"user_tz":-180},"id":"MarK65zCun6r"},"outputs":[],"source":["import numpy as np\n","import h5py\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import matplotlib.gridspec as gridspec\n","import cv2\n","from tensorflow.keras.layers import Input,Dense,Reshape,Conv2D,Dropout,multiply,Dot,Concatenate,subtract,ZeroPadding2D\n","from tensorflow.keras.layers import BatchNormalization,LeakyReLU,Flatten\n","from tensorflow.keras.layers import Conv2DTranspose as Deconv2d\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from keras import backend as K\n","from sklearn.utils import shuffle\n","import tensorflow as tf\n","import keras\n","from keras import layers\n","import os\n","from tqdm import tqdm\n","import re\n","from keras.preprocessing.image import img_to_array"]},{"cell_type":"markdown","metadata":{"id":"xUrOwAtZ2P9j"},"source":["# **Access the Dataset from Drive**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XnKKS8QQ9-Ni"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zd-QbLV5-Hnt"},"outputs":[],"source":["import os\n","folder_path = '/content/drive/MyDrive/archive/'\n","print(f\"Contents of '{folder_path}': {os.listdir(folder_path)}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Prv0p_a9un6x"},"outputs":[],"source":["# To get the files in proper order\n","\n","def sorted_alphanumeric(data):\n","    convert = lambda text: int(text) if text.isdigit() else text.lower()\n","    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)',key)]\n","    return sorted(data,key = alphanum_key)\n","\n","# defining the size of the image\n","SIZE = 256\n","color_img = []\n","path = '/content/drive/MyDrive/archive/landscape Images/color'\n","files = os.listdir(path)\n","files = sorted_alphanumeric(files)\n","for i in tqdm(files):\n","        if i == '2200.jpg':       # Termination bcz there is 7000+ images\n","            break\n","        else:\n","            img = cv2.imread(path + '/'+i,1)\n","\n","            # open cv reads images in BGR format so we have to convert it to RGB\n","            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","            #resizing image\n","            img = cv2.resize(img, (SIZE, SIZE))\n","            img = img.astype('float32') / 255.0\n","            color_img.append(img_to_array(img))\n","\n","\n","gray_img = []\n","path = '/content/drive/MyDrive/archive/landscape Images/gray'\n","files = os.listdir(path)\n","files = sorted_alphanumeric(files)\n","for i in tqdm(files):\n","         if i == '2200.jpg':\n","            break\n","         else:\n","            img = cv2.imread(path + '/'+i,1)\n","            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","            #resizing image\n","            img = cv2.resize(img, (SIZE, SIZE))\n","            img = img.astype('float32') / 255.0\n","            gray_img.append(img_to_array(img))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x7kBHVELun6y"},"outputs":[],"source":["#color_dataset=tf.data.Dataset.from_tensor_slices(np.array(color_img[:2000])).batch(64)\n","#gray_dataset=tf.data.Dataset.from_tensor_slices(np.array(gray_img[:2000])).batch(64)\n","\n","#color_dataset_t=tf.data.Dataset.from_tensor_slices(np.array(color_img[2000:])).batch(8)\n","#gray_dataset_t=tf.data.Dataset.from_tensor_slices(np.array(gray_img[2000:])).batch(8)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eLeSq6SOhmdJ"},"outputs":[],"source":["# Making another dataset from color_image and gray_image to train and test the data.\n","\n","color_dataset = tf.data.Dataset.from_tensor_slices(np.array(color_img[:20])).batch(64)\n","gray_dataset = tf.data.Dataset.from_tensor_slices(np.array(gray_img[:20])).batch(64)\n","\n","color_dataset_t = tf.data.Dataset.from_tensor_slices(np.array(color_img[20:40])).batch(8)\n","gray_dataset_t = tf.data.Dataset.from_tensor_slices(np.array(gray_img[20:40])).batch(8)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a7cFMwwX_Wk_"},"outputs":[],"source":["# The code snippet you provided is using the iter() function and next() function to obtain the next batch of data from the color_dataset and gray_dataset.\n","\n","example_color = next(iter(color_dataset))\n","example_gray = next(iter(gray_dataset))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iRv37jN1un6z"},"outputs":[],"source":["# The provided function plot_images is designed to visualize batches of color and grayscale images side by side.\n","\n","def plot_images(a = 4):\n","\n","    for i in range(a):\n","        plt.figure(figsize = (10,10))\n","        plt.subplot(121)\n","        plt.title('color')\n","        plt.imshow(example_color[i] )\n","\n","        plt.subplot(122)\n","        plt.title('gray')\n","        plt.imshow(example_gray[i])\n","        plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sAPjjMUxun60"},"outputs":[],"source":["\n","plot_images(4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OmcZjfF0un60"},"outputs":[],"source":["#The provided code defines two functions, downsample and upsample, commonly used in the architecture of convolutional neural networks (CNNs),\n","# especially in the context of image processing tasks.\n","# These functions are typically employed in the construction of the encoder and decoder parts of a U-Net or similar architectures\n","\n","def downsample(filters, size, apply_batchnorm=True):\n","\n","  result = tf.keras.Sequential()\n","  result.add(tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',kernel_initializer='he_normal', use_bias=False))\n","\n","  if apply_batchnorm:\n","    result.add(tf.keras.layers.BatchNormalization())\n","\n","  result.add(tf.keras.layers.LeakyReLU())\n","  return result\n","\n","def upsample(filters, size, apply_dropout=False):\n","\n","  result = tf.keras.Sequential()\n","  result.add(tf.keras.layers.Conv2DTranspose(filters, size, strides=2,padding='same',kernel_initializer='he_normal',use_bias=False))\n","  result.add(tf.keras.layers.BatchNormalization())\n","\n","  if apply_dropout:\n","      result.add(tf.keras.layers.Dropout(0.5))\n","\n","  result.add(tf.keras.layers.ReLU())\n","  return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SzwruuGxun61"},"outputs":[],"source":["# defining Generator\n","\n","def Generator():\n","  inputs = tf.keras.layers.Input(shape=[256,256,3])\n","\n","# Define the downsampling stack\n","  down_stack = [\n","    downsample(64, 4, apply_batchnorm=False), # (bs, 128, 128, 64)\n","    downsample(128, 4), # (bs, 64, 64, 128)\n","    downsample(256, 4), # (bs, 32, 32, 256)\n","    downsample(512, 4), # (bs, 16, 16, 512)\n","    downsample(512, 4), # (bs, 8, 8, 512)\n","    downsample(512, 4), # (bs, 4, 4, 512)\n","    downsample(512, 4), # (bs, 2, 2, 512)\n","    downsample(512, 4), # (bs, 1, 1, 512)\n","  ]\n","\n","# Define the upsampling stack\n","  up_stack = [\n","    upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n","    upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n","    upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n","    upsample(512, 4), # (bs, 16, 16, 1024)\n","    upsample(256, 4), # (bs, 32, 32, 512)\n","    upsample(128, 4), # (bs, 64, 64, 256)\n","    upsample(64, 4), # (bs, 128, 128, 128)\n","  ]\n","\n","# Initialize the last layer\n","  initializer = tf.random_normal_initializer(0., 0.02)\n","  last = tf.keras.layers.Conv2DTranspose(3, 4,strides=2,padding='same',kernel_initializer=initializer,activation='tanh') # (bs, 256, 256, 3)\n","\n","  x = inputs\n","\n","# Downsampling through the model\n","  skips = []\n","  for down in down_stack:\n","    x = down(x)\n","    skips.append(x)\n","\n","  skips = reversed(skips[:-1])\n","\n","# Upsampling and establishing the skip connections\n","  for up, skip in zip(up_stack, skips):\n","    x = up(x)\n","    x = tf.keras.layers.Concatenate()([x, skip])\n","\n","# Final layer\n","  x = last(x)\n","\n","  return tf.keras.Model(inputs=inputs, outputs=x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5WxtrNfOun62"},"outputs":[],"source":["# Defining Discriminator\n","\n","def Discriminator():\n","\n","# Initialize weights using a random normal distribution\n","  initializer = tf.random_normal_initializer(0., 0.02)\n","\n","# Define input layers for the real and target images\n","  inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')\n","  tar = tf.keras.layers.Input(shape=[256, 256, 3], name='target_image')\n","\n","# Concatenate the input and target images along the channels axis\n","  x = tf.keras.layers.concatenate([inp, tar]) # (bs, 256, 256, channels*2)\n","\n","# Downsample through the discriminator network\n","  down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)\n","  down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n","  down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n","\n"," # Apply zero-padding and convolutional layer\n","  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n","  conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n","                                kernel_initializer=initializer,\n","                                use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n","\n","# Batch normalization and activation\n","  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n","  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n","\n","# Apply additional zero-padding and final convolutional layer\n","  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n","  last = tf.keras.layers.Conv2D(1, 4, strides=1,\n","                                kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n","\n","# Create and return the discriminator model\n","  return tf.keras.Model(inputs=[inp, tar], outputs=last)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MZpI5fdGun62"},"outputs":[],"source":["generator = Generator()\n","generator.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LM3KEfrIun62"},"outputs":[],"source":["discriminator = Discriminator()\n","discriminator.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q4JSE6CFun63"},"outputs":[],"source":["# defined the loss functions and optimizers for your Generative Adversarial Network (GAN)\n","genLoss=[]\n","discLoss=[]\n","\n","# Binary crossentropy los function is commonly used for GANs\n","loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","\n","# Adam optimizer for updating the weights of the generator/discriminator.\n","generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n","discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n","\n","# This constant is used to scale the L1 loss when calculating the total generator loss.\n","LAMBDA = 100\n","\n","#This function calculates the total generator loss,\n","def generator_loss(disc_generated_output, gen_output, target):\n","  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n","\n","# mean absolute error\n","  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n","\n","  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n","  genLoss.append(total_gen_loss)\n","\n","  return total_gen_loss, gan_loss, l1_loss\n","\n","#This function calculates the total discriminator loss.\n","def discriminator_loss(disc_real_output, disc_generated_output):\n","  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n","\n","  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n","\n","  total_disc_loss = real_loss + generated_loss\n","  discLoss.append(total_disc_loss)\n","\n","  return total_disc_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AeuvT4wPun63"},"outputs":[],"source":["# This function, train_step, defines a single training step for your Generative Adversarial Network (GAN)\n","\n","def train_step(input_image, target, epoch):\n","\n"," # Use GradientTape to record the operations for automatic differentiation\n","  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","\n"," # Generate an output using the generator\n","    gen_output = generator(input_image, training=True)\n","\n"," # Calculate the discriminator outputs for real and generated images\n","    disc_real_output = discriminator([input_image, target], training=True)\n","    disc_generated_output = discriminator([input_image, gen_output], training=True)\n","\n","# Calculate generator and discriminator losses\n","    gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n","    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n","\n"," # Calculate gradients with respect to generator and discriminator variables\n","  generator_gradients = gen_tape.gradient(gen_total_loss,\n","                                          generator.trainable_variables)\n","  discriminator_gradients = disc_tape.gradient(disc_loss,\n","                                               discriminator.trainable_variables)\n","\n","  # Apply gradients to update the weights of the generator and discriminator\n","  generator_optimizer.apply_gradients(zip(generator_gradients,\n","                                          generator.trainable_variables))\n","  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n","                                              discriminator.trainable_variables))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RgL2BRDUun63"},"outputs":[],"source":["# allowing to monitor the progress of your model over multiple epochs\n","\n","import time\n","def fit(train_ds, epochs,):\n","  for epoch in range(epochs):\n","    start = time.time()\n","    print(\"Epoch: \", epoch+1)\n","\n","    # Train\n","    for n, (input_image, target) in train_ds.enumerate():\n","      train_step(input_image, target, epoch)\n","    print()\n","    print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,time.time()-start))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YrfEE9tFIdIK"},"outputs":[],"source":["# The generate_images function appears to be used for visualizing the results of your image colorization model\n","\n","def generate_images(model, test_input, tar):\n","\n","  # Generate predictions using the model\n","  prediction = model(test_input, training=True)\n","\n","  # Create a figure for visualization\n","  plt.figure(figsize=(15,15))\n","\n","# Prepare images and titles for display\n","  display_list = [test_input[0], tar[0], prediction[0]]\n","  title = ['Input Image', 'Ground Truth', 'Predicted Image']\n","\n","  # Display the images and titles in a subplot\n","  for i in range(3):\n","    plt.subplot(1, 3, i+1)\n","    plt.title(title[i])\n","\n","    # getting the pixel values between [0, 1] to plot it.\n","    plt.imshow(display_list[i])\n","    plt.axis('off')\n","\n","     # Show the figure with the generated images\n","  plt.show()\n","\n","# Generate and display images for two examples from the datasets\n","for example_input, example_target in tf.data.Dataset.zip((gray_dataset,color_dataset)).take(2):\n","  generate_images(generator, example_input, example_target)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9sqR5QyZJckA"},"outputs":[],"source":["hist=fit(tf.data.Dataset.zip((gray_dataset, color_dataset)), epochs = 10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-bBOM8wRun64"},"outputs":[],"source":["#  visualize the training progress of a GAN (Generative Adversarial Network).\n","\n","import seaborn as sns\n","sns.set(style='whitegrid')\n","plt.figure(figsize=(10,4))\n","plt.plot(genLoss, label=\"Generator Loss\")\n","plt.plot(discLoss, label=\"Discriminator Loss\")\n","plt.title(\"GAN LOSS VALUES\")\n","plt.xlabel(\"Iterations\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d2hoVInEJ8XR"},"outputs":[],"source":["def generate_images(model, test_input, tar):\n","    # Generate predictions using the model\n","    prediction = model(test_input, training=True)\n","\n","    # Set up the figure for displaying images\n","    plt.figure(figsize=(15, 15))\n","\n","    # Prepare the images and titles for display\n","    display_list = [test_input[0], tar[0], prediction[0]]\n","    title = ['Input Image', 'Ground Truth', 'Predicted Image']\n","\n","    # Iterate over the images and titles, and display them\n","    for i in range(3):\n","        plt.subplot(1, 3, i + 1)\n","        plt.title(title[i])\n","\n","        # Clip pixel values to the valid range [0, 1]\n","        img = np.clip(display_list[i], 0, 1)\n","\n","        # Getting the pixel values between [0, 1] to plot it.\n","        plt.imshow(img)\n","        plt.axis('off')\n","\n","    # Show the figure with the generated images\n","    plt.show()\n","\n","# Set up the figure outside the loop\n","plt.figure(figsize=(15, 15))\n","\n","# Iterate over 5 examples and generate/display images\n","for example_input, example_target in tf.data.Dataset.zip((gray_dataset, color_dataset)).take(5):\n","    generate_images(generator, example_input, example_target)\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":0}